# ğŸ–¼ï¸ Image Caption Generator (CNNâ€“LSTM) â€” Generative AI Project

This project implements a **Generative AIâ€“based Image Caption Generator** using a classic **CNNâ€“LSTM encoderâ€“decoder architecture**.  
The model extracts high-level image features using **InceptionV3** and generates natural-language captions using an **LSTM decoder**, demonstrating cross-modal generative capability.

The project aligns with foundational multimodal AI systems such as Show-and-Tell, BLIP, and Visionâ€“Language Transformers.

---

## ğŸ“Œ Project Objective
The objective is to automatically generate human-like text descriptions for images by learning the relationship between:

- **Visual domain** â†’ image features  
- **Language domain** â†’ textual captions  

This is achieved using a CNN encoder and an LSTM decoder trained on a dataset of imageâ€“caption pairs.

---

## ğŸ“ Dataset Details

- **Dataset:** Open Images Captions (Micro) â€” Hugging Face  
- **Samples:** ~4,900 images  
- **Annotations:** One caption per image  
- **Fields:**  
  - `image` â€” RGB input  
  - `text` â€” human-written caption  

### Why this dataset?
- Lightweight and diverse  
- Ideal for training image captioning models on Colab  
- Contains real-world scenes (people, objects, landscapes, indoor/outdoor)

---

## ğŸ” Data Preprocessing

### **Text Preprocessing**
- Lowercasing  
- Removing punctuation and digits  
- Tokenization (top 5,000 words)  
- Sequence padding (max length = 40)  
- Start `<start>` and end `<end>` markers added  

### **Image Preprocessing**
- Resize to (299 Ã— 299)  
- Pass through **InceptionV3** (pretrained on ImageNet)  
- Extract **2048-dimensional** feature embeddings  

These steps ensure aligned visual and textual representations.

---

## ğŸ§± Model Architecture â€” Encoderâ€“Decoder (CNNâ€“LSTM)

### **Encoder â€” InceptionV3**
- Pretrained CNN  
- Extracts semantic image features  
- Output: 2048-dimensional vector  

### **Decoder â€” LSTM Network**
- Embedding layer (256 units)  
- LSTM (256 units)  
- Dense Softmax output layer  
- Trained to predict the **next word** in sequence  

### Fusion
Image embedding + caption embedding â†’ concatenated â†’ fed into LSTM.

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|----------|--------|
| Loss | Categorical Cross-Entropy |
| Optimizer | Adam (LR = 0.001 â†’ 0.0001 during fine-tuning) |
| Batch Size | 32 |
| Epochs | 45 |
| Vocabulary Size | 5,000 |
| Embedding Dim | 256 |
| LSTM Units | 256 |
| Feature Vector Size | 2048 |
| Regularization | Dropout (0.5) |

### Training Behavior
- Loss reduced from **4.8 â†’ 2.24**  
- No overfitting (dropout + moderate architecture size)  
- Smooth convergence  

---

## âœ¨ Decoding Strategy (Caption Generation)

### Used:
- **Greedy decoding (baseline)**
- **Top-K sampling (k = 3â€“5)**
- **Temperature scaling (Ï„ = 0.6)**
- **Early stopping at `<end>`**

### Result:
- Much more fluent, natural, non-repetitive captions  
- Reduced loops (â€œend end end â€¦â€)  

---

## ğŸ“Š Evaluation Results

Evaluation performed using BLEU scores (standard in machine translation & captioning):

| Metric | Score |
|--------|--------|
| **BLEU-1** | **0.294** |
| **BLEU-2** | **0.176** |
| Training Loss | **2.24** |

### Interpretation
- BLEU-1 â‰ˆ 0.29 â†’ Good word-level match  
- BLEU-2 â‰ˆ 0.18 â†’ Moderate phrase-level match  
- Strong alignment between image features and textual semantics  
- Good baseline performance for a CNNâ€“LSTM model trained on a small dataset

---

## ğŸ–¼ï¸ Qualitative Results
Generated captions show:

- Grammatically fluent sentences  
- Correct recognition of **people**, **objects**, **scenes**, **actions**  
- Occasional mistakes due to limited dataset  
- Reduced repetition due to improved decoding  

---

## ğŸ§  Strengths

- Strong cross-modal alignment (image â†’ language)  
- Fluent, human-like sentence generation  
- Good performance on small dataset  
- Efficient: trains in ~2.5 hours on Google Colab (T4 GPU)  
- Forms a foundation for modern multimodal GenAI systems  

---

## âš ï¸ Limitations

- Some semantic drift (â€œman holding cameraâ€ when none exists)  
- Limited vocabulary (5k words)  
- Small dataset restricts generalization  
- Lacks attention mechanism â†’ struggles with fine-grained details  

---

## ğŸš€ Future Improvements

- Add **Bahdanau or Luong Attention**  
- Replace LSTM with a **Transformer decoder (GPT-2, T5)**  
- Train on MS-COCO or Flickr30k for higher BLEU scores  
- Add Grad-CAM visualization for explainability  
- Enable multilingual caption generation  

---

## ğŸ“¦ Summary Table

| Category | Result |
|---------|--------|
| Architecture | CNNâ€“LSTM (Encoderâ€“Decoder) |
| Encoder | InceptionV3 (ImageNet) |
| Decoder | LSTM (256 units) |
| Dataset | 4,900 images |
| Final Loss | 2.24 |
| BLEU-1 | 0.294 |
| BLEU-2 | 0.176 |
| Decoding | Top-K + Temperature |
| Strength | Fluent captions, cross-modal generation |
| Limitation | Dataset too small |
| Application | Assistive tech, VQA, indexing, GenAI research |

---

## ğŸ§± Tech Stack

- Python  
- TensorFlow / Keras  
- NumPy, Pandas  
- Matplotlib  
- Hugging Face Datasets  
- NLTK  
- Google Colab (GPU)  
